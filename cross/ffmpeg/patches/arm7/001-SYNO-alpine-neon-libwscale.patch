diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/arm/Makefile ./libswscale/arm/Makefile
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/arm/Makefile	2018-11-01 14:34:28.000000000 -0400
+++ ./libswscale/arm/Makefile	2019-10-15 22:12:46.432182104 -0400
@@ -1,8 +1,14 @@
-OBJS        += arm/swscale.o                    \
-               arm/swscale_unscaled.o           \
+OBJS        += arm/swscale_unscaled.o           \
 
 NEON-OBJS   += arm/rgb2yuv_neon_32.o
 NEON-OBJS   += arm/rgb2yuv_neon_16.o
 NEON-OBJS   += arm/hscale.o                     \
                arm/output.o                     \
                arm/yuv2rgb_neon.o               \
+
+#for SYNO_ALPINE_NEON_LIBSWSCALE
+ifeq ($(EXPORT_BUILD_TARGET), ALPINE)
+OBJS += arm/swscale_arm.o
+
+CFLAGS += -mfpu=neon-vfpv4 -flax-vector-conversions -std=gnu99
+endif
diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/arm/swscale_arm.c ./libswscale/arm/swscale_arm.c
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/arm/swscale_arm.c	1969-12-31 19:00:00.000000000 -0500
+++ ./libswscale/arm/swscale_arm.c	2019-10-15 22:12:46.436182134 -0400
@@ -0,0 +1,409 @@
+/*
+ * AltiVec-enhanced yuv2yuvX
+ *
+ * based on the equivalent C code in swscale.c
+ *
+ * This file is part of FFmpeg.
+ *
+ * FFmpeg is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU Lesser General Public
+ * License as published by the Free Software Foundation; either
+ * version 2.1 of the License, or (at your option) any later version.
+ *
+ * FFmpeg is distributed in the hope that it will be useful,
+ * but WITHOUT ANY WARRANTY; without even the implied warranty of
+ * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
+ * Lesser General Public License for more details.
+ *
+ * You should have received a copy of the GNU Lesser General Public
+ * License along with FFmpeg; if not, write to the Free Software
+ * Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
+ */
+
+#include <inttypes.h>
+
+#include "config.h"
+#include "libswscale/swscale.h"
+#include "libswscale/swscale_internal.h"
+#include "libavutil/attributes.h"
+#include "libavutil/cpu.h"
+#include <arm_neon.h>
+
+#include <stdio.h>
+
+#define PLD_NEXT(pointer) 				\
+	asm( "pld [%0, #256] \n\t" : : "r"(pointer));
+
+static int ypfirst = 1;
+static int hsfirst = 1;
+
+static inline int16_t filter_once(int filterSize,
+		const int16_t *filter, int16x8_t filterMask ,
+		uint16x8_t rowSrcA, uint16x8_t rowSrcB)
+{
+	int val;
+	int32x4_t mult;
+	int32x2_t aggregate;
+	int16_t filter9;
+	int16x8_t filterVec;
+
+	filterVec = vld1q_s16(filter);
+	if (filterSize > 8)
+		filter9 = filter[8];
+	PLD_NEXT(filter);
+
+	if (filterSize < 8)
+		filterVec = vandq_s16(filterMask,filterVec);
+
+	//now the two filters have 4 16u each.
+	if (filterSize > 8){
+		val = vgetq_lane_u16(rowSrcB,0) * filter9 ;
+	} else
+		val=0;
+
+	mult = vdupq_n_u32(0);
+	mult = vmlal_s16(mult,vget_low_s16(filterVec), vget_low_s16(rowSrcA));
+	mult = vmlal_s16(mult,vget_high_s16(filterVec), vget_high_s16(rowSrcA));
+	aggregate = vpadd_s32(vget_low_s32(mult), vget_high_s32(mult));
+	aggregate = vpadd_s32(aggregate, aggregate);
+	val+=vget_lane_s32(aggregate,0);
+
+	return FFMIN(val >> 7, (1 << 15) - 1);
+}
+
+static inline void hScale8To15_arm_template(SwsContext *c, int16_t *dst, int dstW,
+		const uint8_t *src, const int16_t *filter,
+		const int32_t *filterPos
+		, int filterSize, int stepSize)
+{
+	int i;
+	uint16x8_t rowSrcA, rowSrcB, rowSrcC;
+	int16x8_t filterMask;
+	int16_t filterMask_array[8];
+	int srcCounter=0;
+	int srcCounterLast;
+	const uint8_t *srcPointer=src;
+	int srcCounterNext;
+	int filterMask_start;
+
+	filterMask_start = filterSize > 8 ? 8 : 0;
+	for (i=filterMask_start; i<filterSize-filterMask_start; i++)
+		filterMask_array[i]=-1;
+	for (i=filterSize-filterMask_start; i<8; i++)
+		filterMask_array[i]=0;
+	filterMask=vld1q_u16(filterMask_array);
+
+	if ((uintptr_t)srcPointer&0x7)
+		fprintf(stderr, "ERROR! src Pointer not aligned!(%d)/n",(uintptr_t)srcPointer&0x7);
+	rowSrcA = vmovl_u8(vld1_u8(srcPointer));
+	srcPointer+=8;
+	rowSrcB = vmovl_u8(vld1_u8(srcPointer));
+	srcPointer+=8;
+	i=0;
+	srcCounterNext = filterPos[0];
+	//Loop-entry: a few first pixels might have
+	//0 for "filterpos". handle it outside the loop
+	//so we don't have to check inside the loop.
+	while (!srcCounterNext) {
+		dst[i] = filter_once(filterSize, filter, filterMask, rowSrcA, rowSrcB);;
+		filter +=filterSize;
+		i++;
+		srcCounterNext = filterPos[i];
+		PLD_NEXT(filterPos+i);
+	}
+	srcCounterLast =  filterPos[dstW-filterSize];
+	while (srcCounter < srcCounterLast) {
+		int internalCount;
+		//8-wide buffer
+		rowSrcC = vmovl_u8(vld1_u8(srcPointer));
+		srcPointer+=8;
+		PLD_NEXT(srcPointer);
+
+		for (internalCount=0;internalCount<8/stepSize; internalCount++)
+		{
+			if (stepSize ==1) {
+				srcCounter++;
+				rowSrcA = vextq_u16(rowSrcA, rowSrcB,1);
+				rowSrcB = vextq_u16(rowSrcB, rowSrcC,1);
+				rowSrcC = vextq_u16(rowSrcC, rowSrcC,1);
+			} else {
+				srcCounter+=2;
+				rowSrcA = vextq_u16(rowSrcA, rowSrcB,2);
+				rowSrcB = vextq_u16(rowSrcB, rowSrcC,2);
+				rowSrcC = vextq_u16(rowSrcC, rowSrcC,2);
+			}
+			//*TODO: this "if" is cheating, and works only for shrinking
+			//with unnoticable visual difference
+			//a "while" would be more correct, but costs some performance
+			//TODO: or does it?
+			if (srcCounter >= srcCounterNext) {
+				dst[i] = filter_once(filterSize, filter, filterMask, rowSrcA, rowSrcB);;
+				filter +=filterSize;
+				i++;
+				srcCounterNext = filterPos[i];
+				PLD_NEXT(filterPos+i);
+			}
+		}
+	}
+
+	//Loop-exit: a few last pixels might have
+	//dstw-1 for "filterpos". handle it outside the loop
+	//so we don't have to check inside the loop.
+	while (i<dstW) {
+		dst[i] = filter_once(filterSize, filter, filterMask, rowSrcA, rowSrcB);;
+		filter +=filterSize;
+		i++;
+		srcCounterNext = filterPos[i];
+		PLD_NEXT(filterPos+i);
+	}
+
+}
+
+extern void hScale8To15_c(SwsContext *c, int16_t *dst, int dstW,
+		const uint8_t *src, const int16_t *filter,
+		const int32_t *filterPos, int filterSize);
+
+static void hScale8To15_arm(SwsContext *c, int16_t *dst, int dstW,
+		const uint8_t *src, const int16_t *filter,
+		const int32_t *filterPos, int filterSize)
+{
+	if (filterSize == 6)
+		hScale8To15_arm_template(c, dst, dstW, src, filter, filterPos, 6, 1);
+	else if (filterSize == 8)
+		hScale8To15_arm_template(c, dst, dstW, src, filter, filterPos, 8, 1);
+	else if (filterSize == 9)
+		hScale8To15_arm_template(c, dst, dstW, src, filter, filterPos, 9, 2);
+	else {
+		hScale8To15_c(c, dst, dstW, src, filter, filterPos, filterSize);
+		if(hsfirst) {
+			fprintf(stderr, "filtersize not supported by hScale8To15_arm! (%d)\n",filterSize);
+			hsfirst = 0;
+		}
+		return;
+	}
+	if(hsfirst) {
+		fprintf(stderr, "filtersize supported by hScale8To15_arm! (%d)\n",filterSize);
+		hsfirst = 0;
+	}
+}
+
+static inline void yuv2planeX_8_arm_template(const int16_t *filter, int filterSize,
+		const int16_t **src, uint8_t *dest, int dstW,
+		const uint8_t *dither, int offset)
+{
+	int i;
+
+	int32x4_t valsA, valsB;
+
+	int16x4_t destA_t, destB_t;
+	int16x8_t dest_t;
+	uint8x8_t destVec;
+	int32x4_t ditherA, ditherB;
+	int16x4_t srcLoader;
+	int16x4_t filterVecA, filterVecB, filterVecC;
+
+	ditherA=vdupq_n_u32(0);
+	ditherB=vdupq_n_u32(0);
+	ditherA=vsetq_lane_s16(dither[(offset + 0) & 7] << 12,ditherA,0);
+	ditherA=vsetq_lane_s16(dither[(offset + 1) & 7] << 12,ditherA,1);
+	ditherA=vsetq_lane_s16(dither[(offset + 2) & 7] << 12,ditherA,2);
+	ditherA=vsetq_lane_s16(dither[(offset + 3) & 7] << 12,ditherA,3);
+	ditherB=vsetq_lane_s16(dither[(offset + 4) & 7] << 12,ditherB,0);
+	ditherB=vsetq_lane_s16(dither[(offset + 5) & 7] << 12,ditherB,1);
+	ditherB=vsetq_lane_s16(dither[(offset + 6) & 7] << 12,ditherB,2);
+	ditherB=vsetq_lane_s16(dither[(offset + 7) & 7] << 12,ditherB,3);
+
+	filterVecA = vld1_s16(filter);
+	if (filterSize > 4)
+		filterVecB = vld1_s16(filter+4);
+	if (filterSize > 8)
+		filterVecC = vld1_s16(filter+8);
+
+	if (filterSize < 12)
+		filterVecC=vset_lane_s16(0,filterVecC,3);
+	if (filterSize < 11)
+		filterVecC=vset_lane_s16(0,filterVecC,2);
+	if (filterSize < 10)
+		filterVecC=vset_lane_s16(0,filterVecC,1);
+	if (filterSize < 9)
+		filterVecC=vset_lane_s16(0,filterVecC,0);
+	if (filterSize < 8)
+		filterVecB=vset_lane_s16(0,filterVecB,3);
+	if (filterSize < 7)
+		filterVecB=vset_lane_s16(0,filterVecB,2);
+	if (filterSize < 6)
+		filterVecB=vset_lane_s16(0,filterVecB,1);
+	if (filterSize < 5)
+		filterVecB=vset_lane_s16(0,filterVecB,0);
+	if (filterSize < 4)
+		filterVecA=vset_lane_s16(0,filterVecA,3);
+	if (filterSize < 3)
+		filterVecA=vset_lane_s16(0,filterVecA,2);
+	if (filterSize < 2)
+		filterVecA=vset_lane_s16(0,filterVecA,1);
+
+	for (i=0; i<dstW; i+=8) {
+		valsA = ditherA;
+		valsB = ditherB;
+
+		srcLoader=vld1_s16(src[0]+i);
+		valsA = vmlal_lane_s16(valsA, srcLoader, filterVecA, 0);
+		srcLoader=vld1_s16(src[0]+i+4);
+		valsB = vmlal_lane_s16(valsB, srcLoader, filterVecA, 0);
+		PLD_NEXT(src[0]+i);
+		if (filterSize > 1) {
+			srcLoader=vld1_s16(src[1]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecA, 1);
+			srcLoader=vld1_s16(src[1]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecA, 1);
+			PLD_NEXT(src[1]+i);
+		}
+		if (filterSize > 2) {
+			srcLoader=vld1_s16(src[2]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecA, 2);
+			srcLoader=vld1_s16(src[2]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecA, 2);
+			PLD_NEXT(src[2]+i);
+		}
+		if (filterSize > 3) {
+			srcLoader=vld1_s16(src[3]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecA, 3);
+			srcLoader=vld1_s16(src[3]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecA, 3);
+			PLD_NEXT(src[3]+i);
+		}
+		if (filterSize > 4) {
+			srcLoader=vld1_s16(src[4]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecB, 0);
+			srcLoader=vld1_s16(src[4]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecB, 0);
+			PLD_NEXT(src[4]+i);
+		}
+		if (filterSize > 5) {
+			srcLoader=vld1_s16(src[5]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecB, 1);
+			srcLoader=vld1_s16(src[5]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecB, 1);
+			PLD_NEXT(src[5]+i);
+		}
+		if (filterSize > 6) {
+			srcLoader=vld1_s16(src[6]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecB, 2);
+			srcLoader=vld1_s16(src[6]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecB, 2);
+			PLD_NEXT(src[6]+i);
+		}
+		if (filterSize > 7) {
+			srcLoader=vld1_s16(src[7]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecB, 3);
+			srcLoader=vld1_s16(src[7]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecB, 3);
+			PLD_NEXT(src[7]+i);
+		}
+		if (filterSize > 8) {
+			srcLoader=vld1_s16(src[8]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecC, 0);
+			srcLoader=vld1_s16(src[8]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecC, 0);
+			PLD_NEXT(src[8]+i);
+		}
+		if (filterSize > 9) {
+			srcLoader=vld1_s16(src[9]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecC, 1);
+			srcLoader=vld1_s16(src[9]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecC, 1);
+			PLD_NEXT(src[9]+i);
+		}
+		if (filterSize > 10) {
+			srcLoader=vld1_s16(src[10]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecC, 2);
+			srcLoader=vld1_s16(src[10]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecC, 2);
+			PLD_NEXT(src[10]+i);
+		}
+		if (filterSize > 11) {
+			srcLoader=vld1_s16(src[11]+i);
+			valsA = vmlal_lane_s16(valsA, srcLoader, filterVecC, 3);
+			srcLoader=vld1_s16(src[11]+i+4);
+			valsB = vmlal_lane_s16(valsB, srcLoader, filterVecC, 3);
+			PLD_NEXT(src[11]+i);
+		}
+
+		valsA=vshrq_n_s32(valsA,16);
+		valsB=vshrq_n_s32(valsB,16);
+
+		destA_t = vshrn_n_s32(valsA, 3);
+		destB_t = vshrn_n_s32(valsB, 3);
+		dest_t = vcombine_s16(destA_t,destB_t);
+		destVec = vqmovun_s16(dest_t);
+		vst1_u8(dest+i, destVec);
+	}
+}
+
+extern void yuv2planeX_8_c(const int16_t *filter, int filterSize,
+		const int16_t **src, uint8_t *dest, int dstW,
+		const uint8_t *dither, int offset);
+
+static void yuv2planeX_8_arm(const int16_t *filter, int filterSize,
+		const int16_t **src, uint8_t *dest, int dstW,
+		const uint8_t *dither, int offset)
+{
+	if (filterSize==1)
+		yuv2planeX_8_arm_template(filter, 1, src, dest, dstW, dither, offset);
+	else if (filterSize==2)
+		yuv2planeX_8_arm_template(filter, 2, src, dest, dstW, dither, offset);
+	else if (filterSize==3)
+		yuv2planeX_8_arm_template(filter, 3, src, dest, dstW, dither, offset);
+	else if (filterSize==4)
+		yuv2planeX_8_arm_template(filter, 4, src, dest, dstW, dither, offset);
+	else if (filterSize==5)
+		yuv2planeX_8_arm_template(filter, 5, src, dest, dstW, dither, offset);
+	else if (filterSize==6)
+		yuv2planeX_8_arm_template(filter, 6, src, dest, dstW, dither, offset);
+	else if (filterSize==7)
+		yuv2planeX_8_arm_template(filter, 7, src, dest, dstW, dither, offset);
+	else if (filterSize==8)
+		yuv2planeX_8_arm_template(filter, 8, src, dest, dstW, dither, offset);
+	else if (filterSize==9)
+		yuv2planeX_8_arm_template(filter, 9, src, dest, dstW, dither, offset);
+	else if (filterSize==10)
+		yuv2planeX_8_arm_template(filter, 10, src, dest, dstW, dither, offset);
+	else if (filterSize==11)
+		yuv2planeX_8_arm_template(filter, 11, src, dest, dstW, dither, offset);
+	else if (filterSize==12)
+		yuv2planeX_8_arm_template(filter, 12, src, dest, dstW, dither, offset);
+	else {
+		yuv2planeX_8_c(filter, filterSize, src, dest, dstW, dither, offset);
+		if(ypfirst) {
+			fprintf(stderr, "filtersize not supported in yuv2planeX_8_arm! (%d)\n",filterSize);
+			ypfirst = 0;
+		}
+		return;
+	}
+	if(ypfirst) {
+		fprintf(stderr, "filtersize supported in yuv2planeX_8_arm! (%d)\n",filterSize);
+		ypfirst = 0;
+	}
+}
+
+av_cold void ff_sws_init_swscale_arm(SwsContext *c)
+{
+	if ((c->srcBpc == 8) && (c->dstBpc <= 14)) {
+		c->hyScale = c->hcScale = hScale8To15_arm;
+	}
+	fprintf(stderr, "Not using hScale8To15_arm c->srcBpc: %d, c->dstBpc:%d\n", c->srcBpc, c->dstBpc);
+}
+
+void ff_sws_init_output_funcs_arm(SwsContext *c,
+		yuv2planarX_fn *yuv2planeX)
+{
+	enum AVPixelFormat dstFormat = c->dstFormat;
+
+	if (is16BPS(dstFormat)) {
+		fprintf(stderr, "Not using yuv2planeX_8_arm is16BPS \n");
+	} else if (isNBPS(dstFormat)) {
+		fprintf(stderr, "Not using yuv2planeX_8_arm is9_OR_10BPS \n");
+	} else {
+		*yuv2planeX=yuv2planeX_8_arm;
+	}
+}
diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/output.c ./libswscale/output.c
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/output.c	2019-08-05 16:52:21.000000000 -0400
+++ ./libswscale/output.c	2019-10-15 22:12:46.436182134 -0400
@@ -36,6 +36,8 @@
 #include "swscale.h"
 #include "swscale_internal.h"
 
+#include "synoconfig.h"
+
 DECLARE_ALIGNED(8, const uint8_t, ff_dither_2x2_4)[][8] = {
 {  1,   3,   1,   3,   1,   3,   1,   3, },
 {  2,   0,   2,   0,   2,   0,   2,   0, },
@@ -377,7 +379,15 @@ yuv2NBPS(14, LE, 0, 10, int16_t)
 yuv2NBPS(16, BE, 1, 16, int32_t)
 yuv2NBPS(16, LE, 0, 16, int32_t)
 
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+void yuv2planeX_8_c(const int16_t *filter, int filterSize,
+                           const int16_t **src, uint8_t *dest, int dstW,
+                           const uint8_t *dither, int offset);
+
+void yuv2planeX_8_c(const int16_t *filter, int filterSize,
+#else
 static void yuv2planeX_8_c(const int16_t *filter, int filterSize,
+#endif
                            const int16_t **src, uint8_t *dest, int dstW,
                            const uint8_t *dither, int offset)
 {
diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/swscale.c ./libswscale/swscale.c
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/swscale.c	2019-07-08 13:45:26.000000000 -0400
+++ ./libswscale/swscale.c	2019-10-15 22:12:46.436182134 -0400
@@ -36,6 +36,8 @@
 #include "swscale_internal.h"
 #include "swscale.h"
 
+#include "synoconfig.h"
+
 DECLARE_ALIGNED(8, const uint8_t, ff_dither_8x8_128)[9][8] = {
     {  36, 68,  60, 92,  34, 66,  58, 90, },
     { 100,  4, 124, 28,  98,  2, 122, 26, },
@@ -122,7 +124,15 @@ static void hScale16To15_c(SwsContext *c
 }
 
 // bilinear / bicubic scaling
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+void hScale8To15_c(SwsContext *c, int16_t *dst, int dstW,
+                          const uint8_t *src, const int16_t *filter,
+                          const int32_t *filterPos, int filterSize);
+
+void hScale8To15_c(SwsContext *c, int16_t *dst, int dstW,
+#else
 static void hScale8To15_c(SwsContext *c, int16_t *dst, int dstW,
+#endif
                           const uint8_t *src, const int16_t *filter,
                           const int32_t *filterPos, int filterSize)
 {
@@ -498,6 +508,10 @@ static int swscale(SwsContext *c, const
              * this array's tail */
             ff_sws_init_output_funcs(c, &yuv2plane1, &yuv2planeX, &yuv2nv12cX,
                                      &yuv2packed1, &yuv2packed2, &yuv2packedX, &yuv2anyX);
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+            if (ARCH_ARM) /* Should add check for neon existence */
+                ff_sws_init_output_funcs_arm(c, &yuv2planeX);
+#endif
             use_mmx_vfilter= 0;
             ff_init_vscale_pfn(c, yuv2plane1, yuv2planeX, yuv2nv12cX,
                            yuv2packed1, yuv2packed2, yuv2packedX, yuv2anyX, use_mmx_vfilter);
@@ -570,9 +584,12 @@ static av_cold void sws_init_swscale(Sws
                              &c->yuv2nv12cX, &c->yuv2packed1,
                              &c->yuv2packed2, &c->yuv2packedX, &c->yuv2anyX);
 
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+    if (ARCH_ARM) /* Should add check for neon existence */
+        ff_sws_init_output_funcs_arm(c, &c->yuv2planeX);
+#endif
     ff_sws_init_input_funcs(c);
 
-
     if (c->srcBpc == 8) {
         if (c->dstBpc <= 14) {
             c->hyScale = c->hcScale = hScale8To15_c;
@@ -605,9 +622,10 @@ SwsFunc ff_getSwsFunc(SwsContext *c)
         ff_sws_init_swscale_x86(c);
     if (ARCH_AARCH64)
         ff_sws_init_swscale_aarch64(c);
-    if (ARCH_ARM)
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+    if (ARCH_ARM) /* Should add check for neon existence */
         ff_sws_init_swscale_arm(c);
-
+#endif
     return swscale;
 }
 
diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/swscale_internal.h ./libswscale/swscale_internal.h
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/swscale_internal.h	2019-08-05 16:52:21.000000000 -0400
+++ ./libswscale/swscale_internal.h	2019-10-15 22:12:46.436182134 -0400
@@ -33,6 +33,8 @@
 #include "libavutil/pixdesc.h"
 #include "libavutil/ppc/util_altivec.h"
 
+#include "synoconfig.h"
+
 #define STR(s) AV_TOSTRING(s) // AV_STRINGIFY is too long
 
 #define YUVRGB_TABLE_HEADROOM 512
@@ -871,6 +873,10 @@ void ff_sws_init_swscale_ppc(SwsContext
 void ff_sws_init_swscale_vsx(SwsContext *c);
 void ff_sws_init_swscale_x86(SwsContext *c);
 void ff_sws_init_swscale_aarch64(SwsContext *c);
+#if defined(SYNO_ALPINE_NEON_LIBSWSCALE)
+void ff_sws_init_output_funcs_arm(SwsContext *c,
+                              yuv2planarX_fn *yuv2planeX);
+#endif
 void ff_sws_init_swscale_arm(SwsContext *c);
 
 void ff_hyscale_fast_c(SwsContext *c, int16_t *dst, int dstWidth,
diff -uprN ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/utils.c ./libswscale/utils.c
--- ../ffmpeg-4.2.1-orig-fix-vaapi_encode/libswscale/utils.c	2019-08-05 16:52:21.000000000 -0400
+++ ./libswscale/utils.c	2019-10-15 22:12:46.436182134 -0400
@@ -1670,8 +1670,12 @@ av_cold int sws_init_context(SwsContext
 #endif /* HAVE_MMXEXT_INLINE */
         {
             const int filterAlign = X86_MMX(cpu_flags)     ? 4 :
+#ifdef SYNO_ALPINE_NEON_LIBSWSCALE
+                                    PPC_ALTIVEC(cpu_flags) ? 8 : 1;
+#else
                                     PPC_ALTIVEC(cpu_flags) ? 8 :
                                     have_neon(cpu_flags)   ? 8 : 1;
+#endif
 
             if ((ret = initFilter(&c->hLumFilter, &c->hLumFilterPos,
                            &c->hLumFilterSize, c->lumXInc,
@@ -1697,8 +1701,12 @@ av_cold int sws_init_context(SwsContext
     /* precalculate vertical scaler filter coefficients */
     {
         const int filterAlign = X86_MMX(cpu_flags)     ? 2 :
+#ifdef SYNO_ALPINE_NEON_LIBSWSCALE
+                                PPC_ALTIVEC(cpu_flags) ? 8 : 1;
+#else
                                 PPC_ALTIVEC(cpu_flags) ? 8 :
                                 have_neon(cpu_flags)   ? 2 : 1;
+#endif
 
         if ((ret = initFilter(&c->vLumFilter, &c->vLumFilterPos, &c->vLumFilterSize,
                        c->lumYInc, srcH, dstH, filterAlign, (1 << 12),
diff -uprN ../ffmpeg-4.2.2-patch030/synoconfig.h ./synoconfig.h
--- ../ffmpeg-4.2.2-patch030/synoconfig.h	2020-01-27 19:36:22.336168627 -0500
+++ ./synoconfig.h	2020-01-27 19:43:00.550402172 -0500
@@ -66,3 +66,12 @@
  * See Media Server #386
  */
 #define SYNO_WRITE_RIFF_INFO_TAG_TO_WAV
+
+/* Use neon acceleration in libswscale, which boosts transcoding
+ * performance on Alpine platform.
+ * The patch is provided by AnnapurnaLabs.
+ * See Video Station #1858
+ */
+#if defined(SYNO_VIDEOSTATION) && defined(SYNO_ALPINE_NEON)
+#define SYNO_ALPINE_NEON_LIBSWSCALE
+#endif
